{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bc4d93",
   "metadata": {},
   "source": [
    "# Ejercicio Práctico RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a214d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd8b0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1479e36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Genera un RDD (newRDD) con la siguiente lista [1, 2, 3, 4, 5]\n",
    "data = [1,2,3,4,5]\n",
    "newRDD = sc.parallelize(data)\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc25ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiplica por 2 todos los elementos del RDD anterior\n",
    "newRDD2= newRDD.map(lambda x: x*2)\n",
    "newRDD2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a047081a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "myRDD= sc.parallelize(data)\n",
    "\n",
    "## Filtra el RDD anterior por los elementos pares\n",
    "rrd = myRDD.filter(lambda x: x%2 == 0)\n",
    "rrd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeb368f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 3, 5, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2= [1, 2, 3, 3, 4, 4, 5, 9]\n",
    "\n",
    "myRDD= sc.parallelize(data2)\n",
    "\n",
    "## Muestra los elementos unicos del RDD\n",
    "\n",
    "myRDD.distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787c7986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 6)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "myRDD = sc.parallelize([('a', 1), ('a', 2), ('a', 3), ('b', 1)])\n",
    "\n",
    "## Obten la suma de los valores agrupados por el key\n",
    "myRDD = myRDD.reduceByKey(add)\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5133d6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 6), ('b', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ordena los RDDs en base al key\n",
    "newRDD= myRDD.sortByKey()\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfe2d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= [1, 2, 3, 4, 5]\n",
    "\n",
    "myRDD= sc.parallelize(data)\n",
    "## Toma los elementos del RDD para multiplicarlos entre si y obtener un resultado\n",
    "myRDD.reduce(lambda x, y: x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6139be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Python', 3), ('Scala', 1), ('R', 2), ('Java', 1)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= ['Python', 'Scala', 'Python', 'R', 'Python', 'Java', 'R' ]\n",
    "\n",
    "myRDD= sc.parallelize(data)\n",
    "\n",
    "## Cuenta cuantas veces aparece cada valor\n",
    "myRDD.countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "633f7716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('b', 1), ('c', 1)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 51909)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\env\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\env\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\env\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\env\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"C:\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\env\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data= [('a', 1), ('b', 1), ('c', 1), ('a', 1)]\n",
    "myRDD = sc.parallelize(data)\n",
    "\n",
    "## Cuenta cuantas veces aparece cada una de las keys\n",
    "myRDD.countByKey().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cea10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "944c19f52a24f95874af1a0965923405492a6b817585bc3c2ef5cb7556d69262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
