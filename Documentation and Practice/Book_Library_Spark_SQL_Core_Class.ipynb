{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2735c3",
   "metadata": {},
   "source": [
    "# Spark SQL \n",
    "### Core Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995607cf",
   "metadata": {},
   "source": [
    "### class pyspark.sql.SparkSession(sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbf2ea",
   "metadata": {},
   "source": [
    "The entry point to programming Spark with the Dataset and DataFrame API. A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bcef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04df5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version : 3.2.4\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Example Erarser\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "print(\"Spark Version : \"+spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3151db2",
   "metadata": {},
   "source": [
    "# class pyspark.sql.DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[SQLContext, SparkSession])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd94cb",
   "metadata": {},
   "source": [
    " A distributed collection of data grouped into named columns. A DataFrame is equivalent to a relational table in Spark SQL, and can be created using various functions in SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83ac7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|  PySpark|\n",
      "|  2|       ML|\n",
      "|  3|Spark SQL|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To create DataFrame using SparkSession\n",
    "department = spark.createDataFrame([\n",
    "    {\"id\": 1, \"name\": \"PySpark\"},\n",
    "    {\"id\": 2, \"name\": \"ML\"},\n",
    "    {\"id\": 3, \"name\": \"Spark SQL\"}\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83d852",
   "metadata": {},
   "source": [
    "### pyspark.sql.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1c538",
   "metadata": {},
   "source": [
    "##### class pyspark.sql.Column(jc: py4j.java_gateway.JavaObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b57cff",
   "metadata": {},
   "source": [
    "#### A column in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f560cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "     [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
    "df.age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb36e27",
   "metadata": {},
   "source": [
    "### pyspark.sql.Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97783e9",
   "metadata": {},
   "source": [
    "Class to observe (named) metrics on a DataFrame.\n",
    "\n",
    "Metrics are aggregation expressions, which are applied to the DataFrame while it is being processed by an action.\n",
    "\n",
    "The metrics have the following guarantees:\n",
    "\n",
    "    It will compute the defined aggregates (metrics) on all the data that is flowing through the Dataset during the action.\n",
    "\n",
    "    It will report the value of the defined aggregate columns as soon as we reach the end of the action.\n",
    "\n",
    "The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that contain references to the input Dataset’s columns must always be wrapped in an aggregate function.\n",
    "\n",
    "An Observation instance collects the metrics while the first action is executed. Subsequent actions do not modify the metrics returned by Observation.get. Retrieval of the metric via Observation.get blocks until the first action has finished and metrics become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOTA: NO LEE LA FUNCION\n",
    "from pyspark.sql.functions import col, count, lit, max\n",
    "\n",
    "from pyspark.sql import Observation\n",
    "\n",
    "df = spark.createDataFrame([[\"Alice\", 2], [\"Bob\", 5]], [\"name\", \"age\"])\n",
    "\n",
    "observation = Observation(\"my metrics\")\n",
    "\n",
    "observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
    "\n",
    "observed_df.count()\n",
    ">>>2\n",
    "\n",
    "observation.get\n",
    ">>>{'count': 2, 'max(age)': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5938b",
   "metadata": {},
   "source": [
    "### pyspark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e69a6",
   "metadata": {},
   "source": [
    " class pyspark.sql.Row[source]\n",
    "\n",
    "    A row in DataFrame. The fields in it can be accessed:\n",
    "        like attributes (row.key)\n",
    "        like dictionary values (row[key])\n",
    "    key in row will search through row keys.\n",
    "\n",
    "    Row can be used to create a row object by using named arguments.\n",
    "    It is not allowed to omit a named argument to represent that the value is None or missing.\n",
    "    This should be explicitly set to None in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0eab1c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(name='Alice', age=11)\n",
      "Alice 11\n",
      "Alice 11\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print(row)\n",
    "Row(name='Alice', age=11)\n",
    "print(row['name'], row['age'])\n",
    "print(row.name, row.age)\n",
    "print('name' in row)\n",
    "'wrong_key' in row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c842494",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrameReader.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a1fdd",
   "metadata": {},
   "source": [
    "DataFrameReader.csv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, None] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, None] = None, inferSchema: Union[bool, str, None] = None, ignoreLeadingWhiteSpace: Union[bool, str, None] = None, ignoreTrailingWhiteSpace: Union[bool, str, None] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, None] = None, maxCharsPerColumn: Union[str, int, None] = None, maxMalformedLogPerPartition: Union[str, int, None] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, None] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, None] = None, enforceSchema: Union[bool, str, None] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, None] = None, recursiveFileLookup: Union[bool, str, None] = None, modifiedBefore: Union[bool, str, None] = None, modifiedAfter: Union[bool, str, None] = None, unescapedQuoteHandling: Optional[str] = None) → DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loads a CSV file and returns the result as a DataFrame.\n",
    "\n",
    "This function will go through the input once to determine \n",
    "the input schema if inferSchema is enabled. \n",
    "To avoid going through the entire data once, disable inferSchema option \n",
    "or specify the schema explicitly using schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66cd517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "|100|null|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
    "df.write.mode(\"overwrite\").format(\"csv\").save(\"Carpeta_temporal/Ejemplo_csv\")\n",
    "# Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
    "spark.read.csv(\"Carpeta_temporal/Ejemplo_csv\", schema=df.schema, nullValue=\"Hyukjin Kwon\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56e64c",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrameReader.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec04b3b",
   "metadata": {},
   "source": [
    "Specifies the input data source format. \n",
    "Write a DataFrame into a JSON file and read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc2d1f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|age|        name|\n",
      "+---+------------+\n",
      "|100|Hyukjin Kwon|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a DataFrame into a JSON file\n",
    "spark.createDataFrame(\n",
    "    [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
    ").write.mode(\"overwrite\").format(\"json\").save(\"Carpeta_temporal/Ejemplo_json\")\n",
    "\n",
    "# Read the JSON file as a DataFrame.\n",
    "spark.read.format('json').load(\"Carpeta_temporal/Ejemplo_json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512967f8",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrameReader.jdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48942775",
   "metadata": {},
   "source": [
    "###  DataFrameReader.jdbc(url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, None] = None, upperBound: Union[str, int, None] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) → DataFrame[source]\n",
    "Construct a DataFrame representing the database table named table accessible via JDBC \n",
    "URL url and connection properties.\n",
    "Partitions of the table will be retrieved in parallel \n",
    "if either column or predicates is specified. \n",
    "lowerBound, upperBound and numPartitions is needed when column is specified.\n",
    "If both column and predicates are specified, column will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41cb79",
   "metadata": {},
   "source": [
    "#### More information of parameters:\n",
    "    \n",
    "URL : https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.jdbc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7faca86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\", \";\").load(\"data/registro_factura.csv\")#Mobiles_Dataset_(2025).csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efa1fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+-------+-----------------+----------------+\n",
      "|Supermecado|Unidad|            Producto|Importe|Precio por Unidad|           Fecha|\n",
      "+-----------+------+--------------------+-------+-----------------+----------------+\n",
      "|  Mercadona|     1|Bicarbonato Blanq...|   1.10|             1.10|17/02/2025_11:53|\n",
      "|     Eroski|     1|       Lacon Ahumado|      3|                3|08/02/2025_12:26|\n",
      "|     Eroski|     1|       Sprite 0,33 l|   0.65|             0.65|08/02/2025_12:26|\n",
      "|     Eroski|     1|    Pepsi Zero Limao|   0.75|             0.75|08/02/2025_12:26|\n",
      "|     Eroski|     1|       Tiburon Pasta|   0.80|             0.80|  08/02/25_12:26|\n",
      "|     Eroski|     2|  Aceitunas Rellenas|   1.94|             0.97|  08/02/25_12:26|\n",
      "|     Eroski|     4|       Amstel Radler|   3.08|             0.77|  08/02/25_12:26|\n",
      "|     Eroski|     1|    Gel WC Eucalipto|   0.93|             0.93|  08/02/25_12:26|\n",
      "|     Eroski|     1|Jabon lavaplatos ...|   1.43|             1.43|  08/02/25_12:26|\n",
      "|     Eroski|     1|          Bolsa Caja|   0.15|             0.15|  08/02/25_12:26|\n",
      "+-----------+------+--------------------+-------+-----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca32775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
